Lab 5 notebook 
Question 1
What do you notice about how this function has split the string "Okay, okay, ladies, now let's get in formation, cause I slay"? What has it done that isn't quite right, and why has it done this? Write down your response in your notes document.
One interesting aspect of the string I noticed, which we also mentioned in class, was that the function separated the “s” in “let’s,” as it thought they were two separate words. This is important to note because for future use of python with other texts in English that use contractions, this could change the composition of the string and how we analyze the data. Another interesting note about punctuation is that instead of leaving, “I” capitalized, in the string it copied it as lower-case. 
Hint: Look at the regular expression being used in this line of code from the function:
words = re.split("\W+", any_chunk_of_text.lower())
What does the regular expression "\W+" mean in Python? To find this out, you can search the re package documentation. Search the page (command/control + F in a web browser) for the term \W. What does it mean? Do the same for the + sign? What does it do? So what do these symbols mean when used together in a regular expression?
/W+ in Python means that it matches any character that is not a word character. It is the opposite of /w. The + means that is causes the RE that comes up to match 1 or more of the repetitions that comes before the RE. When used together in a regular expression they can help to create a universal function for any type of text to follow the same pattern. For example, since the /W matches any character that is not a word, then and the + matches the repetition before, then we can write that as an expression to run a text that may contain numeric or symbolic characters, without repeating the same expression every time if it recognizes the repetions that came before. 
Question 2
What happened? Did it work as you expected? If not, what happened that you didn't expect? Write down your response in your notes document.
It did work as expected when I typed in a sentence I made up in quotation marks. It separated all the words with commas inside the brackets. 
Question 3
Describe the output of this script (the dataframe that displays after the above cell finishes running). Remember that this is the same output as the "vir-ver-counts-specific" spreadsheet in our Lab5 Google drive folder, only for just 10 texts. What is this dataframe showing us? Write down your response in your notes document.
The dataframe is showing us how many times “virtue” with an “I” or an “e” appears in various texts. This information also includes the date of publication, author name, and file title name. This is a more efficient way to analyze a set of data, really of any size, because you can visually see the numeric count of what you’re looking for. Maybe before studying the data someone may have thought that “virtue” was more common in a certain text, but after looking at the data, you could come up with another conclusion. The fact that it also shows the date is important to contextualize the time period and spelling changes from one year or decade to the next. 
Question 4
Look at the below lines from the compare_counts_specific function above. These lines use regular expressions to do something to the value of the <date> field in an xml file (if the contents of the <date> field meet certain conditions, that is). What are these lines doing? 
I think the lines are showing us how the years are organized in the chart, or how the code came together to allow Python to organize them into the chart. We are using a regular expression to make sure that the dates match correctly and are not repeated. 

How lab 5 relates to the readings for week 6
The readings for week 6 connect to the lab in various ways. Both the information from the “Difficult Heritage and the Complexities of Indigenous Data” piece the problematics of responsibly analyzing data was addressed. This connects to any data relating to cultural studies we may use Python to help us gather and sort data. From the beginning of this article, what struck me was how the authors conceptualized data for various types of scholars, from data scientists to anthropologists, and historians. For this article, they specifically looked at problems when identifying and analyzing indigenous data cultures, above all how to do so responsibly. This also applies to the Colored Conventions Project as well. These projects deal with the impacts of racism, colonialism, and White Supremacy. Although these projects do not necessarily use programs such as Python, they could benefit from programming tools. The article on the complexities data suggests that a lot of the cultural items they work with have been digitized, so it would be interesting to potentially develop a dataset and use programming tools to perhaps make an inventory of what they have or to show certain cultural themes in various primary and secondary documents. The same could be said for the CCP, such as a list of different cities where these conventions took place or historical landmarks still existent today could be a great tool for students, researchers, historians, and people in those communities today to learn more about the amazing initiative. 
Based on the readings for this week and my experience with Lab 5, I am now more aware of how I can use this tool in my own research. For example, as I mainly do research relating to Latin America, often I am including topics related to Indigenous groups in the region and the discrimination they have faced and continue to face. This brings to light the issues of cultural sensitivity they mention Likewise, the issue of “difficult heritage,” in that many cultural documents collected through a colonialist lens that do not take consider deeper cultural significance. Therefore, if there is any consideration to use certain programming tools like Python to collect and interpret data based on these groups, it should be done with permission and for educative purposes that can positively influence the cultural legacy of these people for people both inside and outside these communities. There should be a neutrality of data in humanities research, which can be achived through the use of tools like the ones we have used in class thus far. As we learned through the Data Feminism book, there can be some ethical problems when analyzing data for a certain cause, but there is something refreshingly objective about simply looking at the numbers presented in front of us to prove a point or draw attention in a responsible way. 
Data set for Lab 6 
This is the open Github link for the data: https://github.com/xpmethod/torn-apart-open-data 
I plan on using the “Torn apart/separados” data set. This comes from the list you provided us as possible options. 
